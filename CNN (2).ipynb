{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using cNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras) (1.10.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras) (1.0.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "from keras.models import Sequential # used to initialise NN\n",
    "from keras.layers import Convolution2D # used to process images which are 2D unlike videos which 3D \n",
    "from keras.layers import MaxPooling2D #used for pooling\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3...)`\n"
     ]
    }
   ],
   "source": [
    "# nb_filter is the number of features detector or filters, you have no of row and then no of coloumns \n",
    "# border mode - how the borders are handled , not important, default\n",
    "# input shape - all images dont have the same format , 3 channels - RGB , 256 , 256 pixels - \n",
    "# we are using only 64, 64 so it can be faster since we have no gpu# Activation func to obtain non linearity\n",
    "# Activation func to obtain non linearity\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special step: Another convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# we dont need input_shape previous pooled maps would be given automatically\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3. Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening all the pooling maps into one same single vector\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4. Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128, activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, activation=\"sigmoid\")`\n"
     ]
    }
   ],
   "source": [
    "# hidden layer nodes: i/p + o/p/2 , or from experimentation , commmon practice to choose number from power of 2\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "# output layer sigmoid- because the output could be cat or dog\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "# binary outcome so binary_crossentropy\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r  gs://deep-learning-26/notebooks/jupyter/test_set dataset\n",
    "#!gsutil -m cp -r  gs://deep-learning-26/notebooks/jupyter/training_set dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/envs/py3env/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=25, validation_steps=800, steps_per_epoch=8000, validation_data=<keras_pre...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 1172s 147ms/step - loss: 0.3364 - acc: 0.8425 - val_loss: 0.7154 - val_acc: 0.8050\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 1110s 139ms/step - loss: 0.0771 - acc: 0.9718 - val_loss: 1.0054 - val_acc: 0.7901\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 1119s 140ms/step - loss: 0.0430 - acc: 0.9849 - val_loss: 1.1556 - val_acc: 0.7910\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 1103s 138ms/step - loss: 0.0314 - acc: 0.9893 - val_loss: 1.3117 - val_acc: 0.7944\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 1101s 138ms/step - loss: 0.0255 - acc: 0.9916 - val_loss: 1.4525 - val_acc: 0.7925\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 1097s 137ms/step - loss: 0.0217 - acc: 0.9929 - val_loss: 1.3624 - val_acc: 0.8056\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 1099s 137ms/step - loss: 0.0203 - acc: 0.9935 - val_loss: 1.4329 - val_acc: 0.8006\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 1093s 137ms/step - loss: 0.0175 - acc: 0.9946 - val_loss: 1.6562 - val_acc: 0.7965\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 1096s 137ms/step - loss: 0.0160 - acc: 0.9950 - val_loss: 1.5361 - val_acc: 0.8079\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 1094s 137ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 1.5560 - val_acc: 0.8089\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 1096s 137ms/step - loss: 0.0132 - acc: 0.9961 - val_loss: 1.5189 - val_acc: 0.8037\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 1098s 137ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 1.5669 - val_acc: 0.8069\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 1101s 138ms/step - loss: 0.0114 - acc: 0.9966 - val_loss: 1.5981 - val_acc: 0.8015\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 1100s 137ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 1.7363 - val_acc: 0.8009\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 1090s 136ms/step - loss: 0.0110 - acc: 0.9967 - val_loss: 1.6647 - val_acc: 0.8135\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 1091s 136ms/step - loss: 0.0098 - acc: 0.9971 - val_loss: 1.6172 - val_acc: 0.8059\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 1097s 137ms/step - loss: 0.0101 - acc: 0.9970 - val_loss: 1.7271 - val_acc: 0.7901\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 1100s 137ms/step - loss: 0.0088 - acc: 0.9975 - val_loss: 1.6917 - val_acc: 0.8071\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 1097s 137ms/step - loss: 0.0084 - acc: 0.9976 - val_loss: 1.8096 - val_acc: 0.8074\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 1093s 137ms/step - loss: 0.0094 - acc: 0.9975 - val_loss: 1.7041 - val_acc: 0.8154\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 1104s 138ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 1.7726 - val_acc: 0.8061\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 1100s 137ms/step - loss: 0.0070 - acc: 0.9979 - val_loss: 1.7677 - val_acc: 0.8092\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 1100s 137ms/step - loss: 0.0080 - acc: 0.9977 - val_loss: 1.8436 - val_acc: 0.8111\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 1104s 138ms/step - loss: 0.0074 - acc: 0.9979 - val_loss: 1.8653 - val_acc: 0.8112\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 1104s 138ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 1.9049 - val_acc: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ebded3f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image augmentation: 8000 images is not great. More images or a trick. The trick is data augmentation\n",
    "# augmentation tilts the images changes it and trains in batches\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                        steps_per_epoch=8000,\n",
    "                        nb_epoch=25,\n",
    "                        validation_data=test_set,\n",
    "                        nb_val_samples=800)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "predict_image = image.load_img('dataset/single_predictions/cat_or_dog_3.jpeg', target_size = (64, 64))\n",
    "predict_image = image.img_to_array(predict_image)\n",
    "predict_image = np.expand_dims(predict_image, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(predict_image) \n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "if y_pred[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else: \n",
    "  prediction = 'cat'\n",
    "\n",
    "print(prediction)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
